{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSLt1-42rWxE"
   },
   "source": [
    "### **Linking Storage Drive**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lD3wobXKqr-f",
    "outputId": "fb5730a7-516a-456e-bfb4-9a6977426992"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-B6ea86VOmGK"
   },
   "outputs": [],
   "source": [
    "root = '/LayoutTransformer/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WC6MGpirp9N"
   },
   "source": [
    "### **Imports**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-dcOiEoOrVqq"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as k\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jp4KLhN7J23d"
   },
   "source": [
    "### **GPU Setup**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcMPzjTTJ3At"
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrxAk-K7sPwB"
   },
   "source": [
    "### **Importing Data**\n",
    "\n",
    "Data format:\n",
    "  [ Number of samples x Number of Boxes x [Class,X,Y,W,H] ]\n",
    "\n",
    "*   PublayNet\n",
    "*   Rico\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghN8p4iQrplX"
   },
   "outputs": [],
   "source": [
    "publaynet_data = np.load(root+'Data/publaynet.npy')\n",
    "rico_data = np.load(root+'Data/rico_new.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udEOmzv2HdGN"
   },
   "source": [
    "### **Layers**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epBvDnI5HdO6"
   },
   "outputs": [],
   "source": [
    "class MMHSALayer(Layer):\n",
    "    '''\n",
    "    **Masked Multiheaded Self Attention Layer**\n",
    "\n",
    "    heads : Specify the number of heads\n",
    "    '''\n",
    "    def __init__(self,heads=8):\n",
    "        super(MMHSALayer, self).__init__()\n",
    "        self.heads = heads\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.model_dim = input_shape[-2]\n",
    "        self.k = self.add_weight(shape=(self.heads,self.model_dim,self.model_dim),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"Key\")\n",
    "        self.q = self.add_weight(shape=(self.heads,self.model_dim,self.model_dim),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"Query\")\n",
    "        self.v = self.add_weight(shape=(self.heads,self.model_dim,self.model_dim),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"Value\")\n",
    "        self.o = self.add_weight(shape=(self.model_dim,self.model_dim*self.heads),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"Heads\")\n",
    "\n",
    "    def call(self,inputs):\n",
    "        mask_shape = inputs.shape[-1] \n",
    "\n",
    "        mask_0 = np.ones((mask_shape,mask_shape))\n",
    "        for i in range(mask_shape):\n",
    "            for j in range(mask_shape):\n",
    "                if (i>j):\n",
    "                    mask_0[i][j]=0\n",
    "        self.mask_0 = tf.constant(mask_0,dtype=tf.float32)\n",
    "\n",
    "        mask_inf = np.zeros((mask_shape,mask_shape))\n",
    "        for i in range(mask_shape):\n",
    "            for j in range(mask_shape):\n",
    "                if (i>j):\n",
    "                    mask_inf[i][j]=-10000000000\n",
    "        self.mask_inf = tf.constant(mask_inf,dtype=tf.float32)\n",
    "\n",
    "        inputs = tf.expand_dims(inputs,1)\n",
    "\n",
    "        key=tf.matmul(self.k,inputs)\n",
    "        que=tf.matmul(self.q,inputs)\n",
    "        val=tf.matmul(self.v,inputs)\n",
    "\n",
    "        Z=tf.matmul(tf.transpose(key,perm=[0,1,3,2]),que)*(1/np.sqrt(self.model_dim))\n",
    "        W=tf.multiply(Z,self.mask_0)\n",
    "        W=tf.add(W,self.mask_inf)\n",
    "        W=tf.keras.activations.softmax(W,axis=1)\n",
    "        W=tf.multiply(W,self.mask_0)\n",
    "        W=tf.matmul(val,W)\n",
    "\n",
    "        W = tf.reshape(W,(inputs.shape[0],self.model_dim*self.heads,mask_shape))\n",
    "\n",
    "        ans = W\n",
    "\n",
    "        ans = tf.matmul(self.o,ans)\n",
    "        ans=tf.expand_dims(ans,0)\n",
    "\n",
    "        ans = tf.squeeze(ans,axis=0)\n",
    "\n",
    "        return ans\n",
    "\n",
    "class Dense2D(Layer):\n",
    "    '''\n",
    "    **2-Dimensional Dense Layer**\n",
    "    Applies dense layer column-wise (shared weights). Returns the column size of units.\n",
    "\n",
    "    units : Specify the number of output units (column length)\n",
    "    '''\n",
    "    def __init__(self,units):\n",
    "        super(Dense2D, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        input_len = input_shape[-2]\n",
    "\n",
    "        self.w = self.add_weight(shape=(self.units,input_len),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"dense2dw\")\n",
    "\n",
    "    def call(self,inputs,activation = None):\n",
    "\n",
    "        ans = tf.matmul(self.w,inputs)\n",
    "\n",
    "        return ans\n",
    "\n",
    "class FFLayer(Layer):\n",
    "    '''\n",
    "    **Feed Forward Layer**\n",
    "    Applies dense layer column-wise (shared weights), followed by a ReLU Layer, followed by another dense layer column-wise (shared weights). Returns the same column size.\n",
    "\n",
    "    dff : Specify the number of units (column length) in the middle layer \n",
    "    dropout : Dropout Rate\n",
    "    '''\n",
    "    def __init__(self, dff=2048, dropout=0.1):\n",
    "        super(FFLayer,self).__init__()\n",
    "        self.dff = dff \n",
    "        self.dropout = dropout \n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.dropout = tf.keras.layers.Dropout(self.dropout)\n",
    "        self.dout = input_shape[-2]\n",
    "\n",
    "        self.w1 = self.add_weight(shape=(self.dff,self.dout),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"ffw1\")\n",
    "        self.w2 = self.add_weight(shape=(self.dout,self.dff),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"ffw2\")\n",
    "        self.b1 = self.add_weight(shape=(self.dff,1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"ffb1\")\n",
    "        self.b2 = self.add_weight(shape=(self.dout,1),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                 name=\"ffb2\")\n",
    "\n",
    "    def call(self,inputs):\n",
    "\n",
    "        ans = tf.add(tf.matmul(self.w1,inputs),self.b1)\n",
    "        ans = tf.keras.activations.relu(ans)\n",
    "        ans = tf.add(tf.matmul(self.w2,ans),self.b2)\n",
    "\n",
    "        ans = self.dropout(ans)\n",
    "  \n",
    "        return ans\n",
    "\n",
    "class ANLayer(Layer):\n",
    "    '''\n",
    "    **Add and Normalize Layer**\n",
    "    Adds and then Normalizes column wise.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(ANLayer,self).__init__()\n",
    "        self.Normal = tf.keras.layers.LayerNormalization(axis=1)\n",
    "\n",
    "    def call(self,inputs1,inputs2):\n",
    "        sum = tf.add(inputs1,inputs2)\n",
    "        ans=self.Normal(sum)\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJrGCXu4suIp"
   },
   "source": [
    "### **Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6cLgJ9tsuPH"
   },
   "outputs": [],
   "source": [
    "class LTModel(Model):\n",
    "    def __init__(self, input_shape, layers, heads, dff, model_dim, dropout):\n",
    "        super(LTModel, self).__init__()\n",
    "\n",
    "        self.emb = Dense2D(model_dim)\n",
    "\n",
    "        self.SA = []\n",
    "        self.AN1 = []\n",
    "        self.FF = []\n",
    "        self.AN2 = []\n",
    "\n",
    "        for i in range(layers):\n",
    "            self.SA.append(MMHSALayer(heads))\n",
    "            self.AN1.append(ANLayer())\n",
    "            self.FF.append(FFLayer(dff, dropout))\n",
    "            self.AN2.append(ANLayer())\n",
    "\n",
    "        self.deemb = Dense2D(input_shape)\n",
    "        self.sm = tf.keras.layers.Softmax(axis=1)\n",
    "                                                                            \n",
    "    def call(self, x):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for i in range(len(self.SA)):\n",
    "            y = self.SA[i](x)\n",
    "            x = self.AN1[i](x,y)\n",
    "            y = self.FF[i](x)\n",
    "            x = self.AN2[i](x,y)\n",
    "\n",
    "        x = self.deemb(x)\n",
    "        x = self.sm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkuubrSpswZ7"
   },
   "source": [
    "### **Layout Transformer**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EEWk72cFzmwX"
   },
   "outputs": [],
   "source": [
    "class LayoutTransformer:\n",
    "\n",
    "    def __init__(self, n_classes, class_labels=None, n_anchors=(32,32), d=512, n_layers=6, n_heads=8, dff=2048, dropout=0.1):\n",
    "        self.n_classes = n_classes+2\n",
    "        self.n_anchors = n_anchors\n",
    "        self.d = d\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dff = dff\n",
    "        self.dropout = dropout\n",
    "        self.n_row = n_anchors[0]\n",
    "        self.n_col = n_anchors[1]\n",
    "        self.input_dim = 2+n_classes+2*(n_anchors[0]+n_anchors[1])\n",
    "        self.model = LTModel(self.input_dim, model_dim=d, layers=n_layers, heads=n_heads, dff=dff, dropout=dropout)\n",
    "        self.loss_his = []\n",
    "        self.lr_his = []\n",
    "        self.train_data_his = []\n",
    "        if class_labels == None:\n",
    "            self.labels = range(1,n_classes+1)\n",
    "        else:\n",
    "            self.labels = class_labels\n",
    "            \n",
    "        \n",
    "    def compile(self, lr=1e-5):\n",
    "        self.model.compile(loss=tf.keras.losses.KLDivergence(),\n",
    "                           metrics = [tf.keras.losses.KLDivergence()],\n",
    "                           optimizer = tf.keras.optimizers.Adam(learning_rate=lr))\n",
    "        \n",
    "        \n",
    "    def build(self):\n",
    "        self.model.build((1,self.input_dim,1))\n",
    "        \n",
    "\n",
    "    def summary(self):\n",
    "        self.build()\n",
    "        print(self.model.summary())\n",
    "        \n",
    "\n",
    "    def train(self, epochs, batch_size=1, train_data_index=\"All\", rlrop_factor=0.5, rlrop_patience=1000, rlrop_min_delta=0.001):\n",
    "        if train_data_index == \"All\":\n",
    "            train_data_index = range(self.data.shape[0])\n",
    "        rlrop = tf.keras.callbacks.ReduceLROnPlateau(factor=rlrop_factor,patience=rlrop_patience,verbose=1,min_delta=rlrop_min_delta,monitor='kl_divergence')\n",
    "        callbacks = [rlrop]\n",
    "        history = self.model.fit(x=tf.convert_to_tensor(self.x_data[train_data_index]), y=tf.convert_to_tensor(self.y_data[train_data_index]), epochs=epochs, batch_size=batch_size, callbacks=callbacks)\n",
    "        self.loss_his.extend(history.history['loss'])\n",
    "        self.lr_his.extend(history.history['lr'])\n",
    "        for i in range(epochs):\n",
    "            self.train_data_his.append(len(train_data_index))\n",
    "        \n",
    "\n",
    "    def load_weights(self, folder_path, filename):\n",
    "        self.build()\n",
    "        self.model.load_weights(folder_path + '/' + str(filename) + '.h5')\n",
    "        his = json.loads(open(folder_path + '/' + str(filename) + '.json').read())\n",
    "\n",
    "        self.loss_his = his['loss']\n",
    "        self.train_data_his = his['data']\n",
    "        self.lr_his = his['lr']\n",
    "        \n",
    "\n",
    "    def save_weights(self, folder_path, filename):\n",
    "        his = json.dumps({'loss':list(np.array(self.loss_his,dtype='float')),'data':list(np.array(self.train_data_his,dtype='float')),'lr':list(np.array(self.lr_his,dtype='float'))})\n",
    "        open(folder_path + '/' + str(filename) + '.json','w').write(his)\n",
    "        self.model.save_weights(folder_path + '/' + str(filename) + '.h5')\n",
    "        \n",
    "\n",
    "    def predict(self, input):\n",
    "        input = tf.convert_to_tensor(input, dtype='float32')\n",
    "        return self.model(input).numpy()\n",
    "    \n",
    "\n",
    "    def load_data(self, data, rows, cols, e=0.1):\n",
    "        # Make number of boxes equal in eeach document\n",
    "        max_box = 0\n",
    "        for doc in data:\n",
    "            max_box = max(max_box,len(doc))\n",
    "        for doc in range(len(data)):\n",
    "            while (len(data[doc])<max_box):\n",
    "                data[doc].append([self.n_classes-1,0,0,0,0])\n",
    "\n",
    "        self.orig_data = np.array(data,dtype='float32')\n",
    "        data = np.array(data,dtype='float32')\n",
    "\n",
    "        data[:,:,1] = data[:,:,1]/cols*(self.n_col-1)\n",
    "        data[:,:,2] = data[:,:,2]/rows*(self.n_row-1)\n",
    "        data[:,:,3] = data[:,:,3]/cols*(self.n_col-1)\n",
    "        data[:,:,4] = data[:,:,4]/rows*(self.n_row-1)\n",
    "\n",
    "        data = np.array(data,dtype='int')\n",
    "\n",
    "        # Sorting\n",
    "        for i in range(data.shape[0]):\n",
    "            box_num = data[i].shape[0]\n",
    "\n",
    "            c=0\n",
    "            for j in data[i]:\n",
    "                if j[3]==0 and j[4]==0:\n",
    "                    break\n",
    "                c = c+1\n",
    "\n",
    "            order = [*list(data[i][0:c,3].argsort()),*range(c,box_num)] # 4 Width (Col)\n",
    "            data[i] = np.array(data[i,order])\n",
    "            order = [*list(data[i][0:c,4].argsort()),*range(c,box_num)] # 3 Height (Row)\n",
    "            data[i] = np.array(data[i,order])\n",
    "            order = [*list(data[i][0:c,1].argsort()),*range(c,box_num)] # 2 X-Pos (Col)\n",
    "            data[i] = np.array(data[i,order])\n",
    "            order = [*list(data[i][0:c,2].argsort()),*range(c,box_num)] # 1 Y-Pos (Row)\n",
    "            data[i] = np.array(data[i,order])\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "        # One hot encoding\n",
    "        onehot_data = []\n",
    "\n",
    "        for doc in data:\n",
    "            cur_data = []\n",
    "            for box in doc:\n",
    "                cur_cur_data = list(np.zeros(self.input_dim))\n",
    "                cur_cur_data[box[0]] = 1\n",
    "                cur_cur_data[box[1]+self.n_classes] = 1\n",
    "                cur_cur_data[box[2]+self.n_classes+self.n_col] = 1\n",
    "                cur_cur_data[box[3]+self.n_classes+self.n_col+self.n_row] = 1\n",
    "                cur_cur_data[box[4]+self.n_classes+self.n_col*2+self.n_row] = 1\n",
    "                cur_data.append(cur_cur_data)\n",
    "            onehot_data.append(cur_data)\n",
    "\n",
    "        self.onehot_data = np.array(onehot_data, dtype='int')\n",
    "\n",
    "        # x_data with <bos> and y_data with <eos>\n",
    "        x_data = []\n",
    "        y_data = []\n",
    "\n",
    "        for doc in onehot_data:\n",
    "            bos = list(np.zeros(self.input_dim))\n",
    "            bos[0]=1\n",
    "            x = [bos,*doc]\n",
    "            x = np.array(x).T\n",
    "            x_data.append(x)\n",
    "\n",
    "            eos = list(np.zeros(self.input_dim))\n",
    "            eos[self.n_classes-1] = 1\n",
    "            y = [*doc,eos]\n",
    "            for box in y:\n",
    "                for k in range(0, self.n_classes):\n",
    "                    box[k] = (1 - e) * box[k] + e / self.n_classes\n",
    "                for k in range(self.n_classes, self.n_classes+self.n_col):\n",
    "                    box[k] = (1 - e) * box[k] + e / self.n_col\n",
    "                for k in range(self.n_classes+self.n_col, self.n_classes+self.n_col+self.n_row):\n",
    "                    box[k] = (1 - e) * box[k] + e / self.n_row\n",
    "                for k in range(self.n_classes+self.n_col+self.n_row, self.n_classes+2*self.n_col+self.n_row):\n",
    "                    box[k] = (1 - e) * box[k] + e / self.n_col\n",
    "                for k in range(self.n_classes+2*self.n_col+self.n_row, self.n_classes+2*self.n_col+2*self.n_row):\n",
    "                    box[k] = (1 - e) * box[k] + e / self.n_row\n",
    "            y = np.array(y).T\n",
    "            y_data.append(y)\n",
    "\n",
    "        self.x_data = np.array(x_data,dtype=\"float32\")\n",
    "        self.y_data = np.array(y_data,dtype=\"float32\")\n",
    "        \n",
    "\n",
    "    def onehot(self,box, prob=[-1,-1,-1,-1,-1]):\n",
    "        p = 1\n",
    "        c = np.argsort(box[0:self.n_classes],axis=0)[prob[0]]\n",
    "        x = np.argsort(box[self.n_classes:self.n_classes+self.n_col],axis=0)[prob[1]] + self.n_classes\n",
    "        y = np.argsort(box[self.n_classes+self.n_col:self.n_classes+self.n_col+self.n_row],axis=0)[prob[2]] + self.n_classes+self.n_col\n",
    "        w = np.argsort(box[self.n_classes+self.n_col+self.n_row:self.n_classes+2*self.n_col+self.n_row],axis=0)[prob[3]] + self.n_classes+self.n_col+self.n_row\n",
    "        h = np.argsort(box[self.n_classes+2*self.n_col+self.n_row:self.n_classes+2*self.n_col+2*self.n_row],axis=0)[prob[4]] + self.n_classes+2*self.n_col+self.n_row\n",
    "        p = p*box[c][0]*box[x][0]*box[y][0]*box[w][0]*box[h][0]\n",
    "        res = np.zeros((self.input_dim,1))\n",
    "        res[c,0]=1\n",
    "        res[x,0]=1\n",
    "        res[y,0]=1\n",
    "        res[w,0]=1\n",
    "        res[h,0]=1\n",
    "        return (res,p)\n",
    "    \n",
    "\n",
    "    def sort_prob(self,docs):\n",
    "        p = [int(p) for box,p in docs]\n",
    "        p = np.argsort(p)[::-1]\n",
    "        res = []\n",
    "        for i in p:\n",
    "            res.append(docs[i])\n",
    "        return res\n",
    "    \n",
    "\n",
    "    def get_color(self,c):\n",
    "        color_key = [\"#00ffff\",\"#fff5ee\",\"#dc143c\",\"#ffff00\",\"#00ff00\",\"#ff00ff\",\"#1e90ff\",\n",
    "                     \"#ff1493\",\"#8b008b\",\"#ff4500\",\"#8b4513\",\"#808000\",\"#483d8b\",\"#008000\",\n",
    "                     \"#000080\",\"#9acd32\",\"#ffa500\",\"#ba55d3\",\"#00fa9a\",\"#dc143c\",\"#0000ff\",\n",
    "                     \"#f08080\",\"#f0e68c\",\"#dda0dd\",\"#f2dcb3\",\"#f9cfcc\"]\n",
    "        return color_key[int(c)]\n",
    "\n",
    "    def draw_layout(self,ax, doc, prob):\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('green')\n",
    "            spine.set_linewidth(1)\n",
    "        ax.set_xlim(0,self.n_col-1)\n",
    "        ax.set_ylim(0,self.n_row-1)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        for box in doc.T:\n",
    "            c = np.argmax(box[0:self.n_classes],axis=0)\n",
    "            x = np.argmax(box[self.n_classes:self.n_classes+self.n_col],axis=0)\n",
    "            y = np.argmax(box[self.n_classes+self.n_col:self.n_classes+self.n_col+self.n_row],axis=0)\n",
    "            w = np.argmax(box[self.n_classes+self.n_col+self.n_row:self.n_classes+2*self.n_col+self.n_row],axis=0)\n",
    "            h = np.argmax(box[self.n_classes+2*self.n_col+self.n_row:self.n_classes+2*self.n_col+2*self.n_row],axis=0)\n",
    "            r = plt.Rectangle((x,y),w,h, fc=self.get_color(c)+\"72\", ec=self.get_color(c),linewidth=1)\n",
    "            if c==self.n_classes-1:\n",
    "                break\n",
    "\n",
    "            ax.add_patch(r)\n",
    "            \n",
    "\n",
    "    def print_layouts(self,docs,min_boxes,beams_to_print,path=None,ratio_h_w=1.5):\n",
    "        plt.style.use('dark_background')\n",
    "        doc_num = docs[1]\n",
    "        docs = docs[0]\n",
    "        width = beams_to_print+3\n",
    "        height = ratio_h_w*(len(docs))+4\n",
    "        fig = plt.figure(figsize=(width,height),facecolor=\"#000000\",dpi=100)\n",
    "\n",
    "        height_ratios = [0.8/(height),0.9/(height),(height-1.7)/height]\n",
    "        width_ratios = [(width-9)/(2*width),3/width,3/width,3/width,(width-9)/(2*width)]\n",
    "\n",
    "        spec = gridspec.GridSpec(ncols=5, nrows=3,\n",
    "                         width_ratios=width_ratios,\n",
    "                         height_ratios=height_ratios,\n",
    "                         wspace=5/width,left=0.05/width,right=(width-0.05)/width,top=0.98,bottom=0.02,hspace=0.05)\n",
    "        \n",
    "        ax = fig.add_subplot(spec[6])\n",
    "\n",
    "        ax.plot(range(1,len(self.loss_his)+1),self.train_data_his,'-',color='red',linewidth=3)\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_title(\"Train Data\")\n",
    "        \n",
    "        ax = fig.add_subplot(spec[7])\n",
    "\n",
    "        ax.plot(range(1,len(self.loss_his)+1),self.loss_his,'-',color='blue',linewidth=3)\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_title(\"KL Loss\")\n",
    "\n",
    "        ax = fig.add_subplot(spec[8])\n",
    "\n",
    "        ax.plot(range(1,len(self.loss_his)+1),self.lr_his,'-',color='green',linewidth=3)\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_title(\"LR\")\n",
    "        ax.set_yscale(\"log\")\n",
    "\n",
    "        height_ratios = np.ones(len(docs)+2)*ratio_h_w/(height)\n",
    "        height_ratios[0] = 3.8/(height)\n",
    "        height_ratios[1] = 0.2/(height)\n",
    "\n",
    "        spec = gridspec.GridSpec(ncols=1, nrows=len(docs)+2,\n",
    "                         width_ratios=[1],\n",
    "                         height_ratios=height_ratios,\n",
    "                         wspace=0.05,left=0.02,right=0.98,top=0.98,bottom=0.02,hspace=0.05)\n",
    "\n",
    "        ax = fig.add_subplot(spec[0])\n",
    "        ax.axis('off')\n",
    "        ax.invert_yaxis()\n",
    "        ax.text(0.5,0,\"Plot\",ha='center',va='bottom',fontsize=20)\n",
    "        doc_nums=\"\"\n",
    "        for i in doc_num:\n",
    "            doc_nums = doc_nums + \", \" + str(i)\n",
    "        doc_nums = doc_nums[2:]\n",
    "        doc_nums = \"Documents Predicted: \" + doc_nums\n",
    "        props = \"Classes: \" + str(self.n_classes-2) + \"; Epochs: \" + str(len(self.loss_his))\n",
    "        params = \"n_anchors = \" + str(self.n_anchors) + \"; d = \" + str(self.d) + \"; n_layers = \" + str(self.n_layers) + \"; n_heads = \" + str(self.n_heads) + \"; dff = \" + str(self.dff) + \"; dropout = \" + str(self.dropout)\n",
    "        ax.text(0.5,0.03,doc_nums+\"\\n\"+props+\"\\n\"+params,ha='center',va='top',fontsize=10)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        legend = []\n",
    "        legend.append(Patch(facecolor=self.get_color(0)+\"72\", label='<bos>',ec=self.get_color(0),linewidth=1))\n",
    "        for i in range(1,self.n_classes-1):\n",
    "            legend.append(Patch(facecolor=self.get_color(i)+\"72\", label=self.labels[i-1],ec=self.get_color(i),linewidth=1))\n",
    "        legend.append(Patch(facecolor=self.get_color(self.n_classes-1)+\"72\", label='<eos>',ec=self.get_color(self.n_classes-1),linewidth=1))\n",
    "        ax.legend(handles=legend,ncol=5,loc=8)\n",
    "\n",
    "        height_ratios = np.ones(len(docs)+1)*ratio_h_w/(height)\n",
    "        height_ratios[0] = 4/(height)\n",
    "\n",
    "        spec = gridspec.GridSpec(ncols=width, nrows=len(docs)+1,\n",
    "                         width_ratios=np.ones(width),\n",
    "                         height_ratios=height_ratios,\n",
    "                         wspace=0.05,left=0.02,right=0.98,top=0.98,bottom=0.02,hspace=0.05)\n",
    "    \n",
    "        ax = fig.add_subplot(spec[0])\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5,0,\"Ground Truth\",ha='center')\n",
    "\n",
    "        ax = fig.add_subplot(spec[1])\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5,0,\"Input\",ha='center')\n",
    "\n",
    "        ax = fig.add_subplot(spec[2])\n",
    "        ax.axis('off')\n",
    "        ax.text(0.5,0,\"Most Probable\",ha='center')\n",
    "\n",
    "        for i in range(1,beams_to_print+1):\n",
    "            ax = fig.add_subplot(spec[i+2])\n",
    "            ax.axis('off')\n",
    "            ax.text(0.5,0,\"Beam \"+str(i),ha='center')\n",
    "    \n",
    "        for input_count,doc_list in enumerate(docs):\n",
    "\n",
    "            ax = fig.add_subplot(spec[(input_count+1)*width])\n",
    "            self.draw_layout(ax,doc_list[0][0],doc_list[0][1])\n",
    "            ax = fig.add_subplot(spec[(input_count+1)*width+1])\n",
    "            self.draw_layout(ax,doc_list[1][0],doc_list[1][1])\n",
    "            ax = fig.add_subplot(spec[(input_count+1)*width+2])\n",
    "            self.draw_layout(ax,doc_list[2][0],doc_list[2][1])\n",
    "\n",
    "            doc_num=0\n",
    "            for doc in range(3,len(doc_list)):\n",
    "                if doc_num==beams_to_print:\n",
    "                    break\n",
    "                if (len(doc_list[doc][0][0])>=min_boxes):\n",
    "                    ax = fig.add_subplot(spec[(input_count+1)*width+doc])\n",
    "                    self.draw_layout(ax,doc_list[doc][0],doc_list[doc][1])\n",
    "                    doc_num = doc_num+1\n",
    "        if path!=None:\n",
    "            plt.savefig(path, facecolor=\"#000000\")\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    def layout_completion(self, initial_boxes_num=2, data_num_array=[0], beam_length=[1], max_boxes=10):\n",
    "        x = self.x_data[data_num_array,:,0:initial_boxes_num]\n",
    "        res = []\n",
    "\n",
    "        for input_count,input in enumerate(x):\n",
    "            input = np.array([input])\n",
    "            most_prob_doc = [(input,1)]\n",
    "\n",
    "            for step in range(max_boxes):\n",
    "                cur = most_prob_doc.pop(0)\n",
    "                pre = np.array([self.model(cur[0]).numpy()[0,:,-1]])\n",
    "                (box,p) = self.onehot(pre.T,[-1,-1,-1,-1,-1])\n",
    "                p=p*cur[1]\n",
    "                cur_box = np.array([box])\n",
    "                cur_doc = np.append(cur[0],cur_box,axis=2)\n",
    "\n",
    "                most_prob_doc.append((cur_doc,p))\n",
    "\n",
    "                if most_prob_doc[0][0][0,self.n_classes-1,-1]==1:\n",
    "                    break\n",
    "\n",
    "            docs = []\n",
    "            q = [(input,1)]\n",
    "            total_calc = 1\n",
    "            for step in range(max_boxes):\n",
    "                beam = 1\n",
    "                if step<len(beam_length):\n",
    "                    beam = beam_length[step]\n",
    "                \n",
    "                for i in range(len(q)):\n",
    "                    cur_list = []\n",
    "                    cur = q.pop(0)\n",
    "                    for j in ([0] if beam==1 else [0,1,2,4,8,16]):\n",
    "                        prob = []\n",
    "                        temp = j\n",
    "                        for k in range(5):\n",
    "                            prob.insert(0,-1-temp%2)\n",
    "                            temp = int(temp/2)\n",
    "\n",
    "                        pre = np.array([self.model(cur[0]).numpy()[0,:,-1]])\n",
    "                        (box,p) = self.onehot(pre.T,prob)\n",
    "\n",
    "                        p=p*cur[1]\n",
    "                        cur_box = np.array([box])\n",
    "                        cur_doc = np.append(cur[0],cur_box,axis=2)\n",
    "\n",
    "                        cur_list.append((cur_doc,p))\n",
    "                    \n",
    "                    cur_list = self.sort_prob(cur_list)\n",
    "                    for j in range(beam):\n",
    "                        if cur_list[j][0][0,self.n_classes-1,-1]==1:\n",
    "                            docs.append(cur_list[j])\n",
    "                        else:\n",
    "                            q.append(cur_list[j])\n",
    "\n",
    "                    print(\"\\r\"+str(total_calc)+\"  Left in Queue: \"+str(len(q))+\" ; \"+\"Current Shape: \"+str(cur[0].shape)+\" ; \"+\"Docs Prepared: \"+str(len(docs)),end=\"\")\n",
    "                    total_calc = total_calc+1\n",
    "            print(\"\")\n",
    "\n",
    "            docs = self.sort_prob(docs)\n",
    "\n",
    "            res.append([(np.array([self.x_data[data_num_array[input_count]]]),1),(np.array([input[0]]),1),*most_prob_doc,*docs])\n",
    "\n",
    "        return (res,data_num_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Publay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8lGSDNFp6MU"
   },
   "source": [
    "### **Creating Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI2KUXRzPF3r"
   },
   "outputs": [],
   "source": [
    "publay_model = LayoutTransformer(n_classes=6, class_labels=['None','Text','Title','List','Table','Figure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rXFyHPg5gq3"
   },
   "outputs": [],
   "source": [
    "publay_model.load_data(publaynet_data[0:10000],rows=1,cols=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = publay_model\n",
    "\n",
    "epochs = 100\n",
    "lrate = 1e-5\n",
    "\n",
    "# Reduce LR on Plateau\n",
    "min_delta = 0.001\n",
    "patience = 20\n",
    "factor = 0.95\n",
    "\n",
    "count = 0\n",
    "model.compile(lr=lrate)\n",
    "\n",
    "for i in range(epochs):\n",
    "    gc.collect()\n",
    "    k.clear_session()\n",
    "    try:\n",
    "        if model.loss_his[-2]-model.loss_his[-1]<min_delta:\n",
    "            count = count+1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if count==patience:\n",
    "        count = 0\n",
    "        lrate = lrate*factor\n",
    "        model.compile(lr=lrate)\n",
    "        \n",
    "    model.train(epochs=1, batch_size=1, train_data_index='All')\n",
    "    model.save_weights(root+'Publay Weights','model'+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publay_model.load_weights(root+\"Publay Weights\",\"model100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = publay_model.layout_completion(data_num_array=range(10), initial_boxes_num=2, beam_length=[3,3,2], max_boxes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGdlreZO7DmT"
   },
   "outputs": [],
   "source": [
    "publay_model.print_layouts(predictions, min_boxes=2, beams_to_print=10, path=root+'Results/publay.png' ,ratio_h_w=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8lGSDNFp6MU"
   },
   "source": [
    "### **Creating Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI2KUXRzPF3r"
   },
   "outputs": [],
   "source": [
    "rico_model = LayoutTransformer(n_classes=24, class_labels=['Text','Image','Icon','Text Button','List Item','Input','Card','Web View','Radio Button','Drawer','Checkbox','Advertisement','Modal','Pager Indicator','Slider','On/Off Switch','Button Bar','Toolbar','Number Stepper','Multi-Tab','Date Picker','Map View','Video','Bottom Navigation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rXFyHPg5gq3"
   },
   "outputs": [],
   "source": [
    "rico_model.load_data(rico_data[],rows=2560,cols=1440)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rico_model\n",
    "\n",
    "epochs = 100\n",
    "lrate = 1e-5\n",
    "\n",
    "# Reduce LR on Plateau\n",
    "min_delta = 0.001\n",
    "patience = 20\n",
    "factor = 0.95\n",
    "\n",
    "count = 0\n",
    "model.compile(lr=lrate)\n",
    "\n",
    "for i in range(epochs):\n",
    "    gc.collect()\n",
    "    k.clear_session()\n",
    "    try:\n",
    "        if model.loss_his[-2]-model.loss_his[-1]<min_delta:\n",
    "            count = count+1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if count==patience:\n",
    "        count = 0\n",
    "        lrate = lrate*factor\n",
    "        model.compile(lr=lrate)\n",
    "        \n",
    "    model.train(epochs=1, batch_size=1, train_data_index='All')\n",
    "    model.save_weights(root+'Rico Weights','model'+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Model**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rico_model.load_weights(root+\"Rico Weights\",\"model100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Results**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rico_model.layout_completion(data_num_array=range(10), initial_boxes_num=2, beam_length=[3,3,2], max_boxes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGdlreZO7DmT"
   },
   "outputs": [],
   "source": [
    "rico_model.print_layouts(predictions, min_boxes=2, beams_to_print=10, path=root+'Results/rico.png' ,ratio_h_w=1.5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Layout Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
